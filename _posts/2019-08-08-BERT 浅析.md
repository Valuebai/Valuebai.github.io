---
layout:     post					# 使用的布局（不需要改）
title:      BERT 浅析		# 标题
subtitle:   从Word2Vec到Elmo，再到BERT，最后用Keras快速上手   			#副标题
date:       2019-08-08
author:     Valuebai
header-img: img/20191124bg_article_bert.jpg
catalog: true
tags:
    - 深度学习
---

> 原文地址： https://xiaosheng.me/2019/08/07/article161/


2018 年 10 月 11 日，Google AI Language 发布了论文[《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》](https://arxiv.org/pdf/1810.04805.pdf)，其中提出的 BERT 模型在 11 个 NLP 任务上的表现刷新了记录，在自然语言处理学界以及工业界都引起了不小的热议。BERT 的出现，彻底改变了预训练产生词向量和下游具体 NLP 任务的关系，提出龙骨级的训练词向量概念。

本文将简单地横向比较 Word2Vec，ELMo 和 BERT 这三个模型，并给出 Keras 版的使用实例。



## Hello Mr.Lucky


注意点：
1、复制图片过来，要先等一会！要先等一会！！要先等一会！！！让小书匠保存成github的图床图片，先不要保存到github上的文章，不然图片获取用的是：(./images/1561707358500.png)，会导致复制到CSDN上无法使用
2、复制图片过来先保存一遍，不然容易出现(./images/1561707358500.png)



【小书匠 常用快捷键及设置】https://www.cnblogs.com/wushuaishuai/p/9308094.html



【环境】win10_x64 + python3.6.5


【Me】https://github.com/Valuebai/


【参考】
1、出处：地址